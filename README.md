# üöÄ Projeto N8N via Docker

Este projeto permite rodar o [N8N](https://n8n.io/) localmente utilizando Docker, com persist√™ncia de dados, autentica√ß√£o b√°sica, integra√ß√£o com banco de dados Postgres, Redis para filas e o Ollama para IA generativa.

---

## üì¶ Servi√ßos inclusos

- **Postgres**: Banco de dados para persist√™ncia dos workflows e credenciais.
- **Redis**: Gerenciamento de filas para execu√ß√£o dos workflows.
- **Ollama**: API para rodar modelos de IA localmente.
- **N8N**: Plataforma de automa√ß√£o de workflows.
- **Qdrant**: Banco de dados vetorial para embeddings, busca sem√¢ntica e RAG/mem√≥ria.

---

## ‚öôÔ∏è Como levantar o ambiente

1. **Clone o reposit√≥rio e acesse a pasta:**

   ```sh
   git clone <url-do-repo>
   cd N8N
   ```

2. **Suba os containers:**
   ```sh
   docker compose up -d
   ```

---

## üóÑÔ∏è Persist√™ncia de dados

- Os dados do N8N (workflows e credenciais) ficam em `/home/node/.n8n` dentro do container.
- O volume `n8n_data` garante que os dados sejam persistidos mesmo ap√≥s reiniciar os containers.
- Outros volumes:
  - `postgres_data`: dados do Postgres
  - `redis_data`: dados do Redis
  - `ollama_data`: modelos baixados do Ollama
  - `qdrant_data`: cole√ß√µes/vetores armazenados pelo Qdrant

---

## üß† Qdrant: Banco Vetorial

### Por que adicionamos o Qdrant?

O Qdrant √© um banco de dados vetorial utilizado para armazenar e buscar embeddings de alta dimens√£o. Ele permite implementar recursos como busca sem√¢ntica, RAG (Retrieval-Augmented Generation), recomenda√ß√£o e mem√≥ria de longo prazo em workflows do N8N. Com isso, √© poss√≠vel:

- Indexar documentos em vetores e recuperar conte√∫dos semelhantes por significado.
- Construir agentes com mem√≥ria, hist√≥rico e contexto persistente.
- Aumentar respostas de IA com contexto relevante (RAG) e reduzir alucina√ß√µes.

### Como acessar o Qdrant

- **API HTTP**: `http://localhost:6333`
- **gRPC**: `localhost:6334`
- **Volume de dados**: `qdrant_data` mapeado para `/qdrant/storage` (persist√™ncia local)

O servi√ßo est√° definido no `docker-compose.yml` como `qdrant` e o N8N j√° possui a vari√°vel de ambiente `QDRANT_URL` apontando para `http://qdrant:6333` para uso dentro da rede Docker.

### Testes r√°pidos (cURL)

1. Verificar sa√∫de do servi√ßo:

```sh
curl http://localhost:6333/healthz
```

2. Criar uma cole√ß√£o (ex.: `docs`, com vetores de dimens√£o 768 e m√©trica cosseno):

```sh
curl -X PUT "http://localhost:6333/collections/docs" \
  -H "Content-Type: application/json" \
  -d '{
    "vectors": { "size": 768, "distance": "Cosine" }
  }'
```

3. Inserir pontos (embeddings) na cole√ß√£o:

```sh
curl -X PUT "http://localhost:6333/collections/docs/points?wait=true" \
  -H "Content-Type: application/json" \
  -d '{
    "points": [
      {"id": 1, "vector": [0.01, 0.02, 0.03, /* ... 768 dims ... */ 0.04], "payload": {"title": "Doc A"}},
      {"id": 2, "vector": [0.02, 0.03, 0.01, /* ... 768 dims ... */ 0.05], "payload": {"title": "Doc B"}}
    ]
  }'
```

4. Fazer uma busca por similaridade:

```sh
curl -X POST "http://localhost:6333/collections/docs/points/search" \
  -H "Content-Type: application/json" \
  -d '{
    "vector": [0.01, 0.02, 0.03, /* ... 768 dims ... */ 0.04],
    "limit": 5,
    "with_payload": true
  }'
```

Obs.: substitua os vetores de exemplo pelos embeddings reais gerados pelo seu modelo (por exemplo, via Ollama + uma etapa de embedding adequada).

### Integra√ß√£o com o N8N

- Dentro do container do N8N, o Qdrant est√° acess√≠vel via `QDRANT_URL=http://qdrant:6333`.
- Use o n√≥ `HTTP Request` para chamar a API do Qdrant (criar cole√ß√µes, upsert e busca).
- Se estiver utilizando n√≥s/comunidade para vetores, aponte o endpoint para o `QDRANT_URL`.
- Fluxos t√≠picos de RAG: `Texto/Arquivo ‚Üí Embedding ‚Üí Upsert no Qdrant ‚Üí Consulta por vetor ‚Üí Contexto ‚Üí LLM`.

---

## üîí Autentica√ß√£o

- O N8N est√° protegido por autentica√ß√£o b√°sica:
  - **Usu√°rio:** `admin`
  - **Senha:** `senha-super-segura`
- O timezone est√° configurado para `America/Sao_Paulo`.

---

## üåê Acessando o N8N

Abra o navegador em:

üëâ [http://localhost:5678](http://localhost:5678)

---

## ü§ñ Testando o Ollama

### 1Ô∏è‚É£ Listar modelos dispon√≠veis

Dentro do container Ollama, rode:

```sh
docker exec -it <nome_do_container_ollama> ollama list
```

### 2Ô∏è‚É£ Baixar o modelo desejado

Exemplo para baixar o modelo `llama3`:

```sh
docker exec -it <nome_do_container_ollama> ollama pull llama3
```

Voc√™ pode baixar outros modelos, como `mistral`, `codellama`, etc.

### 3Ô∏è‚É£ Testar o modelo

Ap√≥s baixar, teste com o comando:

```sh
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama3",
    "prompt": "Explique o que √© n8n em 3 frases curtas."
  }'
```

Se tudo estiver certo, o modelo ir√° retornar uma resposta. ‚úÖ

---

## üìù Observa√ß√µes

- O servi√ßo N8N depende do Postgres, Redis e Ollama, garantindo que todos estejam prontos antes de iniciar.
- Os workflows podem ser salvos na pasta `./workflows` do projeto, que √© montada no container.

---

## üìö Refer√™ncias

- [Documenta√ß√£o oficial do N8N](https://docs.n8n.io/)
- [Documenta√ß√£o do Ollama](https://ollama.com/)
- [Documenta√ß√£o do Qdrant](https://qdrant.tech/documentation/)
